// Generated by `wit-bindgen` 0.29.0.

///|
let llm_model = "llama2-chat"

///|
let default_prompt = "tell me a joke"

///| `handle` handles a `GET /llm` request and responds with the answer
/// to a fixed prompt. It also handles a `POST /llm` request and sends
/// the body as a prompt to the LLM.
pub fn handle(
  request : @types.IncomingRequest,
  response_out : @types.ResponseOutparam
) -> Unit {
  let response = (match @util.split_path(request) {
      (@types.Get, ["llm"]) => prompt_llm(default_prompt)
      (@types.Post, ["llm"]) =>
        match @util.get_body_bytes(request) {
          Ok(bytes) => @base64.bytes2str(bytes) |> prompt_llm()
          Err(e) =>
            @util.make_response(
              @base64.str2bytes(e.to_string()),
              status_code=500,
            )
        }
      _ => return @util.respond_404(response_out)
    })
    |> Ok
  response_out.set(response)
}

///|
fn prompt_llm(prompt : String) -> @types.OutgoingResponse {
  @util.println("prompting '\{llm_model}' with: '\{prompt}")
  match @llm.infer(llm_model, prompt, None) {
    Ok(result) =>
      @util.make_response(
        @base64.str2bytes(result.to_string()),
        content_type=b"application/json",
      )
    _ => @util.make_404_response()
  }
}
